{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67bbab47-da7c-4f07-9e3f-3ea08fe732a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "/usr/local/lib/python3.11/dist-packages/IPython/core/magics/osm.py:428: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "# %cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfbb62bf-d41b-4836-9d63-a67deb22d3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untitled.ipynb\t dataset.zip  lora2.ipynb\t start.ipynb\n",
      "Untitled1.ipynb  diffusers    lora_dataset.json  test.ipynb\n",
      "dataset\t\t lora.ipynb   sd3_local\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25f2791d-82d7-46c9-aec2-4213b85d6dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'diffusers' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f35611e3-ed8a-4b35-bdf3-4a2fbe6e37f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: diffusers 0.35.0.dev0\n",
      "Uninstalling diffusers-0.35.0.dev0:\n",
      "  Successfully uninstalled diffusers-0.35.0.dev0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip uninstall -y diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b6f2dbe-bbca-4b48-a0d0-9c6e2972cb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CITATION.cff\t    MANIFEST.in    _typos.toml\texamples\tsrc\n",
      "CODE_OF_CONDUCT.md  Makefile\t   benchmarks\tpyproject.toml\ttests\n",
      "CONTRIBUTING.md     PHILOSOPHY.md  docker\tscripts\t\tutils\n",
      "LICENSE\t\t    README.md\t   docs\t\tsetup.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "907dea7e-5bb0-4dfa-9b5b-41813e6f8cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/solar_ml/diffusers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0509594a-2c8f-4fcc-8ce7-53d840c093e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///workspace/solar_ml/diffusers\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: importlib_metadata in /usr/lib/python3/dist-packages (from diffusers==0.35.0.dev0) (4.6.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from diffusers==0.35.0.dev0) (3.13.1)\n",
      "Collecting huggingface-hub>=0.27.0 (from diffusers==0.35.0.dev0)\n",
      "  Downloading huggingface_hub-0.33.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from diffusers==0.35.0.dev0) (1.26.3)\n",
      "Collecting regex!=2019.12.17 (from diffusers==0.35.0.dev0)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from diffusers==0.35.0.dev0) (2.32.3)\n",
      "Collecting safetensors>=0.3.1 (from diffusers==0.35.0.dev0)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from diffusers==0.35.0.dev0) (10.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers==0.35.0.dev0) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers==0.35.0.dev0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers==0.35.0.dev0) (6.0.2)\n",
      "Collecting tqdm>=4.42.1 (from huggingface-hub>=0.27.0->diffusers==0.35.0.dev0)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers==0.35.0.dev0) (4.9.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub>=0.27.0->diffusers==0.35.0.dev0)\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.35.0.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.35.0.dev0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.35.0.dev0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.35.0.dev0) (2024.8.30)\n",
      "Downloading huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\n",
      "Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m140.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m191.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Building wheels for collected packages: diffusers\n",
      "  Building editable for diffusers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for diffusers: filename=diffusers-0.35.0.dev0-0.editable-py3-none-any.whl size=11378 sha256=bd95dd60933048c77b06712151e46077e8832ae01a94368323bfadba5ecd5d18\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-raf1eres/wheels/19/55/95/3ae9174cab06a69d96e108f69eeceff5c1ea0b33456217c2e9\n",
      "Successfully built diffusers\n",
      "Installing collected packages: tqdm, safetensors, regex, hf-xet, huggingface-hub, diffusers\n",
      "Successfully installed diffusers-0.35.0.dev0 hf-xet-1.1.5 huggingface-hub-0.33.4 regex-2024.11.6 safetensors-0.5.3 tqdm-4.67.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b506287-da6a-4202-a3ee-908470a7c21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CITATION.cff\t    MANIFEST.in    _typos.toml\texamples\tsrc\n",
      "CODE_OF_CONDUCT.md  Makefile\t   benchmarks\tpyproject.toml\ttests\n",
      "CONTRIBUTING.md     PHILOSOPHY.md  docker\tscripts\t\tutils\n",
      "LICENSE\t\t    README.md\t   docs\t\tsetup.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3097f68f-be8e-41ce-9340-b8f6c216748e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/solar_ml/diffusers/examples/dreambooth\n"
     ]
    }
   ],
   "source": [
    "%cd examples/dreambooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f70dca3-4f9a-4553-8fd9-336af61631dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate>=0.31.0 (from -r requirements_sd3.txt (line 1))\n",
      "  Downloading accelerate-1.8.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from -r requirements_sd3.txt (line 2)) (0.19.1+cu124)\n",
      "Collecting transformers>=4.41.2 (from -r requirements_sd3.txt (line 3))\n",
      "  Downloading transformers-4.53.2-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting ftfy (from -r requirements_sd3.txt (line 4))\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting tensorboard (from -r requirements_sd3.txt (line 5))\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.11/dist-packages (from -r requirements_sd3.txt (line 6)) (3.1.3)\n",
      "Collecting peft==0.11.1 (from -r requirements_sd3.txt (line 7))\n",
      "  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting sentencepiece (from -r requirements_sd3.txt (line 8))\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft==0.11.1->-r requirements_sd3.txt (line 7)) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.11.1->-r requirements_sd3.txt (line 7)) (24.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.11.1->-r requirements_sd3.txt (line 7)) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft==0.11.1->-r requirements_sd3.txt (line 7)) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.11.1->-r requirements_sd3.txt (line 7)) (2.4.1+cu124)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft==0.11.1->-r requirements_sd3.txt (line 7)) (4.67.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft==0.11.1->-r requirements_sd3.txt (line 7)) (0.5.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.11.1->-r requirements_sd3.txt (line 7)) (0.33.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->-r requirements_sd3.txt (line 2)) (10.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1->-r requirements_sd3.txt (line 7)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1->-r requirements_sd3.txt (line 7)) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1->-r requirements_sd3.txt (line 7)) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1->-r requirements_sd3.txt (line 7)) (3.2.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1->-r requirements_sd3.txt (line 7)) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1->-r requirements_sd3.txt (line 7)) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1->-r requirements_sd3.txt (line 7)) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1->-r requirements_sd3.txt (line 7)) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1->-r requirements_sd3.txt (line 7)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.2.65 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1->-r requirements_sd3.txt (line 7)) (12.4.2.65)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.0.44 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1->-r requirements_sd3.txt (line 7)) (11.2.0.44)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.119 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1->-r requirements_sd3.txt (line 7)) (10.3.5.119)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.0.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1->-r requirements_sd3.txt (line 7)) (11.6.0.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.0.142 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1->-r requirements_sd3.txt (line 7)) (12.3.0.142)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1->-r requirements_sd3.txt (line 7)) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1->-r requirements_sd3.txt (line 7)) (12.4.99)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1->-r requirements_sd3.txt (line 7)) (12.4.99)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1->-r requirements_sd3.txt (line 7)) (3.0.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41.2->-r requirements_sd3.txt (line 3)) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.41.2->-r requirements_sd3.txt (line 3)) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers>=4.41.2->-r requirements_sd3.txt (line 3))\n",
      "  Downloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->-r requirements_sd3.txt (line 4)) (0.2.13)\n",
      "Collecting absl-py>=0.4 (from tensorboard->-r requirements_sd3.txt (line 5))\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard->-r requirements_sd3.txt (line 5))\n",
      "  Downloading grpcio-1.73.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard->-r requirements_sd3.txt (line 5))\n",
      "  Downloading markdown-3.8.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard->-r requirements_sd3.txt (line 5))\n",
      "  Downloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements_sd3.txt (line 5)) (75.1.0)\n",
      "Requirement already satisfied: six>1.9 in /usr/lib/python3/dist-packages (from tensorboard->-r requirements_sd3.txt (line 5)) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->-r requirements_sd3.txt (line 5))\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard->-r requirements_sd3.txt (line 5))\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2->-r requirements_sd3.txt (line 6)) (2.1.5)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.11.1->-r requirements_sd3.txt (line 7)) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.41.2->-r requirements_sd3.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.41.2->-r requirements_sd3.txt (line 3)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.41.2->-r requirements_sd3.txt (line 3)) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.41.2->-r requirements_sd3.txt (line 3)) (2024.8.30)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.13.0->peft==0.11.1->-r requirements_sd3.txt (line 7)) (1.3.0)\n",
      "Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
      "Downloading accelerate-1.8.1-py3-none-any.whl (365 kB)\n",
      "Downloading transformers-4.53.2-py3-none-any.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m281.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m174.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading grpcio-1.73.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m187.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown-3.8.2-py3-none-any.whl (106 kB)\n",
      "Downloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl (321 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m310.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m145.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Installing collected packages: sentencepiece, werkzeug, tensorboard-data-server, protobuf, markdown, grpcio, ftfy, absl-py, tensorboard, tokenizers, transformers, accelerate, peft\n",
      "Successfully installed absl-py-2.3.1 accelerate-1.8.1 ftfy-6.3.1 grpcio-1.73.1 markdown-3.8.2 peft-0.11.1 protobuf-6.31.1 sentencepiece-0.2.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tokenizers-0.21.2 transformers-4.53.2 werkzeug-3.1.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements_sd3.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02173650-1a6f-4c36-acaf-70baa57a1f9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                           Version        Editable project location\n",
      "--------------------------------- -------------- -----------------------------\n",
      "absl-py                           2.3.1\n",
      "accelerate                        1.8.1\n",
      "anyio                             4.6.0\n",
      "argon2-cffi                       23.1.0\n",
      "argon2-cffi-bindings              21.2.0\n",
      "arrow                             1.3.0\n",
      "asttokens                         2.4.1\n",
      "async-lru                         2.0.4\n",
      "attrs                             24.2.0\n",
      "babel                             2.16.0\n",
      "beautifulsoup4                    4.12.3\n",
      "bitsandbytes                      0.46.1\n",
      "bleach                            6.1.0\n",
      "blinker                           1.4\n",
      "certifi                           2024.8.30\n",
      "cffi                              1.17.1\n",
      "charset-normalizer                3.3.2\n",
      "comm                              0.2.2\n",
      "cryptography                      3.4.8\n",
      "dbus-python                       1.2.18\n",
      "debugpy                           1.8.5\n",
      "decorator                         5.1.1\n",
      "defusedxml                        0.7.1\n",
      "diffusers                         0.35.0.dev0    /workspace/solar_ml/diffusers\n",
      "distro                            1.7.0\n",
      "entrypoints                       0.4\n",
      "executing                         2.1.0\n",
      "fastjsonschema                    2.20.0\n",
      "filelock                          3.13.1\n",
      "fqdn                              1.5.1\n",
      "fsspec                            2024.2.0\n",
      "ftfy                              6.3.1\n",
      "grpcio                            1.73.1\n",
      "h11                               0.14.0\n",
      "hf-xet                            1.1.5\n",
      "httpcore                          1.0.5\n",
      "httplib2                          0.20.2\n",
      "httpx                             0.27.2\n",
      "huggingface-hub                   0.33.4\n",
      "idna                              3.10\n",
      "importlib-metadata                4.6.4\n",
      "ipykernel                         6.29.5\n",
      "ipython                           8.27.0\n",
      "ipython-genutils                  0.2.0\n",
      "ipywidgets                        8.1.5\n",
      "isoduration                       20.11.0\n",
      "jedi                              0.19.1\n",
      "jeepney                           0.7.1\n",
      "Jinja2                            3.1.3\n",
      "json5                             0.9.25\n",
      "jsonpointer                       3.0.0\n",
      "jsonschema                        4.23.0\n",
      "jsonschema-specifications         2023.12.1\n",
      "jupyter-archive                   3.4.0\n",
      "jupyter_client                    7.4.9\n",
      "jupyter_contrib_core              0.4.2\n",
      "jupyter_contrib_nbextensions      0.7.0\n",
      "jupyter_core                      5.7.2\n",
      "jupyter-events                    0.10.0\n",
      "jupyter-highlight-selected-word   0.2.0\n",
      "jupyter-lsp                       2.2.5\n",
      "jupyter_nbextensions_configurator 0.6.4\n",
      "jupyter_server                    2.14.2\n",
      "jupyter_server_terminals          0.5.3\n",
      "jupyterlab                        4.2.5\n",
      "jupyterlab_pygments               0.3.0\n",
      "jupyterlab_server                 2.27.3\n",
      "jupyterlab_widgets                3.0.13\n",
      "keyring                           23.5.0\n",
      "launchpadlib                      1.10.16\n",
      "lazr.restfulclient                0.14.4\n",
      "lazr.uri                          1.0.6\n",
      "lxml                              5.3.0\n",
      "Markdown                          3.8.2\n",
      "MarkupSafe                        2.1.5\n",
      "matplotlib-inline                 0.1.7\n",
      "mistune                           3.0.2\n",
      "more-itertools                    8.10.0\n",
      "mpmath                            1.3.0\n",
      "nbclassic                         1.1.0\n",
      "nbclient                          0.10.0\n",
      "nbconvert                         7.16.4\n",
      "nbformat                          5.10.4\n",
      "nest-asyncio                      1.6.0\n",
      "networkx                          3.2.1\n",
      "notebook                          6.5.5\n",
      "notebook_shim                     0.2.4\n",
      "numpy                             1.26.3\n",
      "nvidia-cublas-cu12                12.4.2.65\n",
      "nvidia-cuda-cupti-cu12            12.4.99\n",
      "nvidia-cuda-nvrtc-cu12            12.4.99\n",
      "nvidia-cuda-runtime-cu12          12.4.99\n",
      "nvidia-cudnn-cu12                 9.1.0.70\n",
      "nvidia-cufft-cu12                 11.2.0.44\n",
      "nvidia-curand-cu12                10.3.5.119\n",
      "nvidia-cusolver-cu12              11.6.0.99\n",
      "nvidia-cusparse-cu12              12.3.0.142\n",
      "nvidia-nccl-cu12                  2.20.5\n",
      "nvidia-nvjitlink-cu12             12.4.99\n",
      "nvidia-nvtx-cu12                  12.4.99\n",
      "oauthlib                          3.2.0\n",
      "overrides                         7.7.0\n",
      "packaging                         24.1\n",
      "pandocfilters                     1.5.1\n",
      "parso                             0.8.4\n",
      "peft                              0.16.0\n",
      "pexpect                           4.9.0\n",
      "pillow                            10.2.0\n",
      "pip                               24.2\n",
      "platformdirs                      4.3.6\n",
      "prometheus_client                 0.21.0\n",
      "prompt_toolkit                    3.0.47\n",
      "protobuf                          6.31.1\n",
      "psutil                            6.0.0\n",
      "ptyprocess                        0.7.0\n",
      "pure_eval                         0.2.3\n",
      "pycparser                         2.22\n",
      "Pygments                          2.18.0\n",
      "PyGObject                         3.42.1\n",
      "PyJWT                             2.3.0\n",
      "pyparsing                         2.4.7\n",
      "python-apt                        2.4.0+ubuntu4\n",
      "python-dateutil                   2.9.0.post0\n",
      "python-json-logger                2.0.7\n",
      "PyYAML                            6.0.2\n",
      "pyzmq                             24.0.1\n",
      "referencing                       0.35.1\n",
      "regex                             2024.11.6\n",
      "requests                          2.32.3\n",
      "rfc3339-validator                 0.1.4\n",
      "rfc3986-validator                 0.1.1\n",
      "rpds-py                           0.20.0\n",
      "safetensors                       0.5.3\n",
      "SecretStorage                     3.3.1\n",
      "Send2Trash                        1.8.3\n",
      "sentencepiece                     0.2.0\n",
      "setuptools                        75.1.0\n",
      "six                               1.16.0\n",
      "sniffio                           1.3.1\n",
      "soupsieve                         2.6\n",
      "stack-data                        0.6.3\n",
      "sympy                             1.12\n",
      "tensorboard                       2.19.0\n",
      "tensorboard-data-server           0.7.2\n",
      "terminado                         0.18.1\n",
      "tinycss2                          1.3.0\n",
      "tokenizers                        0.21.2\n",
      "torch                             2.4.1+cu124\n",
      "torchaudio                        2.4.1+cu124\n",
      "torchvision                       0.19.1+cu124\n",
      "tornado                           6.4.1\n",
      "tqdm                              4.67.1\n",
      "traitlets                         5.14.3\n",
      "transformers                      4.53.2\n",
      "triton                            3.0.0\n",
      "types-python-dateutil             2.9.0.20240906\n",
      "typing_extensions                 4.9.0\n",
      "uri-template                      1.3.0\n",
      "urllib3                           2.2.3\n",
      "wadllib                           1.3.6\n",
      "wcwidth                           0.2.13\n",
      "webcolors                         24.8.0\n",
      "webencodings                      0.5.1\n",
      "websocket-client                  1.8.0\n",
      "Werkzeug                          3.1.3\n",
      "wheel                             0.44.0\n",
      "widgetsnbextension                4.0.13\n",
      "zipp                              1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60ecae9f-e84b-473b-81b7-f3210677e0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/root/.cache/huggingface/accelerate/default_config.yaml')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from accelerate.utils import write_basic_config\n",
    "write_basic_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ab26e4b-39ff-4855-84cd-0136b6db1507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f68bc4afb7419da3c60da1e446c8cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26df58c9a8745b2b9d8cdc3f265f629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "alvan-nee-Id1DBHv4fbg-unsplash.jpeg:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94fda85347a245109da8ba9aca8a30c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "alvan-nee-eoqnr8ikwFE-unsplash.jpeg:   0%|          | 0.00/1.17M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc9b2fcc2b604b88891cce87d40a83f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "alvan-nee-bQaAJCbNq3g-unsplash.jpeg:   0%|          | 0.00/1.40M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70657498c2a84427b3528573c9568689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "alvan-nee-9M0tSjb-cpA-unsplash.jpeg:   0%|          | 0.00/677k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "663fc1bc030a48cc89f206a5b9007246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "alvan-nee-brFsZ7qszSY-unsplash.jpeg:   0%|          | 0.00/1.19M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/workspace/solar_ml/diffusers/examples/dreambooth/dog'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from huggingface_hub import snapshot_download\n",
    "\n",
    "# local_dir = \"./dog\"\n",
    "# snapshot_download(\n",
    "#     \"diffusers/dog-example\",\n",
    "#     local_dir=local_dir, repo_type=\"dataset\",\n",
    "#     ignore_patterns=\".gitattributes\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4e5cad0-6ce7-48a9-931e-5cd7bc7ff04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipex flag is deprecated, will be removed in Accelerate v1.10. From 2.7.0, PyTorch has all needed optimizations for Intel CPU and XPU.\n",
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "/usr/bin/python: can't open file '/workspace/solar_ml/train_dreambooth_sd3.py': [Errno 2] No such file or directory\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/accelerate_cli.py\", line 50, in main\n",
      "    args.func(args)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/launch.py\", line 1199, in launch_command\n",
      "    simple_launcher(args)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/launch.py\", line 785, in simple_launcher\n",
      "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
      "subprocess.CalledProcessError: Command '['/usr/bin/python', 'train_dreambooth_sd3.py', '--pretrained_model_name_or_path=', '--instance_data_dir=', '--output_dir=', '--mixed_precision=fp16', '--instance_prompt=a photo of sks dog', '--resolution=1024', '--train_batch_size=1', '--gradient_accumulation_steps=4', '--learning_rate=1e-4', '--report_to=wandb', '--lr_scheduler=constant', '--lr_warmup_steps=0', '--max_train_steps=500', '--validation_prompt=A photo of sks dog in a bucket', '--validation_epochs=25', '--seed=0', '--push_to_hub']' returned non-zero exit status 2.\n"
     ]
    }
   ],
   "source": [
    "!export MODEL_NAME=\"stabilityai/stable-diffusion-3-medium-diffusers\"\n",
    "!export INSTANCE_DIR=\"dog\"\n",
    "!export OUTPUT_DIR=\"trained-sd3\"\n",
    "\n",
    "!accelerate launch train_dreambooth_sd3.py \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME  \\\n",
    "  --instance_data_dir=$INSTANCE_DIR \\\n",
    "  --output_dir=$OUTPUT_DIR \\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --instance_prompt=\"a photo of sks dog\" \\\n",
    "  --resolution=1024 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=4 \\\n",
    "  --learning_rate=1e-4 \\\n",
    "  --report_to=\"wandb\" \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --max_train_steps=500 \\\n",
    "  --validation_prompt=\"A photo of sks dog in a bucket\" \\\n",
    "  --validation_epochs=25 \\\n",
    "  --seed=\"0\" \\\n",
    "  --push_to_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c460009-9c50-4143-b66a-3c5849abb269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8d8563e-7e64-42bf-bfa5-97ba92728743",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf /workspace/solar_ml/diffusers/examples/dreambooth/dog/.ipynb_checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf128ff2-aef8-48df-88c5-729783de6090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # 1) Uninstall any PyPI diffusers and install your local clone in -editable mode\n",
    "# # !pip uninstall -y diffusers\n",
    "# # !pip install -e /workspace/solar_ml/diffusers  # assumes your clone lives here\n",
    "\n",
    "# # # 2) Install the rest of the bits you need\n",
    "# # !pip install -q accelerate transformers datasets peft bitsandbytes\n",
    "\n",
    "# # 3) Force imports to pick up your local diffusers, not the release\n",
    "# import sys\n",
    "# sys.path.insert(0, \"/workspace/solar_ml/diffusers\")                          # your clone\n",
    "# sys.path.append(   \"/workspace/solar_ml/diffusers/examples/dreambooth\")      # the examples folder\n",
    "\n",
    "# # 4) Import & patch\n",
    "# import train_dreambooth_sd3 as tb\n",
    "# from train_dreambooth_sd3 import parse_args, main\n",
    "# from accelerate import notebook_launcher\n",
    "\n",
    "# # 5) Build exactly the same args you'd pass on the CLI\n",
    "# args = parse_args([\n",
    "#     \"--pretrained_model_name_or_path\", \"stabilityai/stable-diffusion-3-medium-diffusers\",\n",
    "#     \"--instance_data_dir\",  \"/workspace/solar_ml/diffusers/examples/dreambooth/dog\",\n",
    "#     \"--output_dir\",         \"./trained-sd3\",\n",
    "#     \"--instance_prompt\",    \"a photo of sks dog\",\n",
    "#     \"--resolution\",         \"1024\",\n",
    "#     \"--train_batch_size\",   \"1\",\n",
    "#     \"--gradient_accumulation_steps\", \"4\",\n",
    "#     \"--learning_rate\",      \"1e-4\",\n",
    "#     \"--max_train_steps\",    \"500\",\n",
    "#     \"--validation_prompt\",  \"a photo of sks dog in a bucket\",\n",
    "#     \"--validation_epochs\",  \"25\",\n",
    "#     \"--mixed_precision\",    \"fp16\",\n",
    "#     \"--report_to\",          \"tensorboard\",\n",
    "# ])\n",
    "\n",
    "# # 6) Monkey‑patch so load_text_encoders() can see args\n",
    "# tb.args = args\n",
    "\n",
    "# # 7) Finally launch your single‑GPU run\n",
    "# notebook_launcher(main, (args,), num_processes=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7018aee-3753-496e-9b15-6149418b89c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: cannot stat '/workspace/solar_ml/diffusers/examples/dreambooth/dog/*.png': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p /workspace/clean_dog\n",
    "!cp /workspace/solar_ml/diffusers/examples/dreambooth/dog/*.{jpeg,png} /workspace/clean_dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86c6ec0c-872a-4240-b3d7-8e16eed0a185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.4.1+cu124)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.2.65 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.2.65)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.0.44 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.2.0.44)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.119 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.5.119)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.0.99 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.6.0.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.0.142 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.3.0.142)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.99)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.99)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m219.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.46.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "804ac2a8-18d9-4e02-89d9-01d567eff31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ipex flag is deprecated, will be removed in Accelerate v1.10. From 2.7.0, PyTorch has all needed optimizations for Intel CPU and XPU.\n",
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "07/15/2025 16:27:07 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type t5 to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'base_shift', 'max_shift', 'use_beta_sigmas', 'stochastic_sampling', 'base_image_seq_len', 'invert_sigmas', 'use_dynamic_shifting', 'shift_terminal', 'use_exponential_sigmas', 'use_karras_sigmas', 'max_image_seq_len', 'time_shift_type'} was not found in config. Values will be initialized to default values.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.33s/it]\n",
      "{'mid_block_add_attention'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "{'dual_attention_layers', 'qk_norm'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing SD3Transformer2DModel.\n",
      "\n",
      "All the weights of SD3Transformer2DModel were initialized from the model checkpoint at stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use SD3Transformer2DModel for predictions without further training.\n",
      "07/15/2025 16:27:41 - INFO - __main__ - ***** Running training *****\n",
      "07/15/2025 16:27:41 - INFO - __main__ -   Num examples = 12\n",
      "07/15/2025 16:27:41 - INFO - __main__ -   Num batches each epoch = 12\n",
      "07/15/2025 16:27:41 - INFO - __main__ -   Num Epochs = 167\n",
      "07/15/2025 16:27:41 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "07/15/2025 16:27:41 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "07/15/2025 16:27:41 - INFO - __main__ -   Gradient Accumulation steps = 4\n",
      "07/15/2025 16:27:41 - INFO - __main__ -   Total optimization steps = 500\n",
      "Steps:   0%|          | 0/500 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "Steps:   1%|          | 3/500 [00:03<09:20,  1.13s/it, loss=0.224, lr=0.0001] \n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.53s/it]\u001b[A\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.27s/it]\u001b[A\n",
      "{'image_encoder', 'feature_extractor'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/9 [00:00<?, ?it/s]\u001b[A{'base_shift', 'max_shift', 'use_beta_sigmas', 'stochastic_sampling', 'base_image_seq_len', 'invert_sigmas', 'use_dynamic_shifting', 'shift_terminal', 'use_exponential_sigmas', 'use_karras_sigmas', 'max_image_seq_len', 'time_shift_type'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as FlowMatchEulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "\n",
      "Loading pipeline components...:  67%|██████▋   | 6/9 [00:00<00:00, 57.02it/s]\u001b[ALoaded tokenizer_3 as T5TokenizerFast from `tokenizer_3` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "Loading pipeline components...: 100%|██████████| 9/9 [00:00<00:00, 30.69it/s]\n",
      "07/15/2025 16:28:07 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: a photo of snow covered solar panels.\n",
      "Steps:  16%|█▌        | 78/500 [02:10<07:52,  1.12s/it, loss=0.0818, lr=0.0001] \n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.82s/it]\u001b[A\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.42s/it]\u001b[A\n",
      "{'image_encoder', 'feature_extractor'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/9 [00:00<?, ?it/s]\u001b[A{'base_shift', 'max_shift', 'use_beta_sigmas', 'stochastic_sampling', 'base_image_seq_len', 'invert_sigmas', 'use_dynamic_shifting', 'shift_terminal', 'use_exponential_sigmas', 'use_karras_sigmas', 'max_image_seq_len', 'time_shift_type'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as FlowMatchEulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "Loaded tokenizer_3 as T5TokenizerFast from `tokenizer_3` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "\n",
      "Loading pipeline components...: 100%|██████████| 9/9 [00:00<00:00, 30.88it/s]\u001b[A\n",
      "07/15/2025 16:30:14 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: a photo of snow covered solar panels.\n",
      "Steps:  31%|███       | 153/500 [04:17<06:13,  1.08s/it, loss=0.0454, lr=0.0001]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.46s/it]\u001b[A\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.21s/it]\u001b[A\n",
      "{'image_encoder', 'feature_extractor'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/9 [00:00<?, ?it/s]\u001b[A{'base_shift', 'max_shift', 'use_beta_sigmas', 'stochastic_sampling', 'base_image_seq_len', 'invert_sigmas', 'use_dynamic_shifting', 'shift_terminal', 'use_exponential_sigmas', 'use_karras_sigmas', 'max_image_seq_len', 'time_shift_type'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as FlowMatchEulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "Loaded tokenizer_3 as T5TokenizerFast from `tokenizer_3` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "\n",
      "Loading pipeline components...: 100%|██████████| 9/9 [00:00<00:00, 30.35it/s]\u001b[A\n",
      "07/15/2025 16:32:19 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: a photo of snow covered solar panels.\n",
      "Steps:  46%|████▌     | 228/500 [06:23<04:59,  1.10s/it, loss=0.114, lr=0.0001]   \n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.52s/it]\u001b[A\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.26s/it]\u001b[A\n",
      "{'image_encoder', 'feature_extractor'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/9 [00:00<?, ?it/s]\u001b[A{'base_shift', 'max_shift', 'use_beta_sigmas', 'stochastic_sampling', 'base_image_seq_len', 'invert_sigmas', 'use_dynamic_shifting', 'shift_terminal', 'use_exponential_sigmas', 'use_karras_sigmas', 'max_image_seq_len', 'time_shift_type'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as FlowMatchEulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "\n",
      "Loading pipeline components...:  67%|██████▋   | 6/9 [00:00<00:00, 57.27it/s]\u001b[ALoaded tokenizer_3 as T5TokenizerFast from `tokenizer_3` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "Loading pipeline components...: 100%|██████████| 9/9 [00:00<00:00, 29.63it/s]\n",
      "07/15/2025 16:34:26 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: a photo of snow covered solar panels.\n",
      "Steps:  61%|██████    | 303/500 [08:30<03:46,  1.15s/it, loss=0.0761, lr=0.0001] \n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.62s/it]\u001b[A\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.33s/it]\u001b[A\n",
      "{'image_encoder', 'feature_extractor'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/9 [00:00<?, ?it/s]\u001b[A{'base_shift', 'max_shift', 'use_beta_sigmas', 'stochastic_sampling', 'base_image_seq_len', 'invert_sigmas', 'use_dynamic_shifting', 'shift_terminal', 'use_exponential_sigmas', 'use_karras_sigmas', 'max_image_seq_len', 'time_shift_type'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as FlowMatchEulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "\n",
      "Loading pipeline components...:  67%|██████▋   | 6/9 [00:00<00:00, 58.87it/s]\u001b[ALoaded tokenizer_3 as T5TokenizerFast from `tokenizer_3` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "Loading pipeline components...: 100%|██████████| 9/9 [00:00<00:00, 29.71it/s]\n",
      "07/15/2025 16:36:35 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: a photo of snow covered solar panels.\n",
      "Steps:  76%|███████▌  | 378/500 [10:48<02:34,  1.27s/it, loss=0.084, lr=0.0001] \n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.62s/it]\u001b[A\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.40s/it]\u001b[A\n",
      "{'image_encoder', 'feature_extractor'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/9 [00:00<?, ?it/s]\u001b[A{'base_shift', 'max_shift', 'use_beta_sigmas', 'stochastic_sampling', 'base_image_seq_len', 'invert_sigmas', 'use_dynamic_shifting', 'shift_terminal', 'use_exponential_sigmas', 'use_karras_sigmas', 'max_image_seq_len', 'time_shift_type'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as FlowMatchEulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "\n",
      "Loading pipeline components...:  67%|██████▋   | 6/9 [00:00<00:00, 56.84it/s]\u001b[ALoaded tokenizer_3 as T5TokenizerFast from `tokenizer_3` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "Loading pipeline components...: 100%|██████████| 9/9 [00:00<00:00, 27.34it/s]\n",
      "07/15/2025 16:38:52 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: a photo of snow covered solar panels.\n",
      "Steps:  91%|█████████ | 453/500 [12:58<00:52,  1.11s/it, loss=0.0273, lr=0.0001]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.48s/it]\u001b[A\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.06s/it]\u001b[A\n",
      "{'image_encoder', 'feature_extractor'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/9 [00:00<?, ?it/s]\u001b[A{'base_shift', 'max_shift', 'use_beta_sigmas', 'stochastic_sampling', 'base_image_seq_len', 'invert_sigmas', 'use_dynamic_shifting', 'shift_terminal', 'use_exponential_sigmas', 'use_karras_sigmas', 'max_image_seq_len', 'time_shift_type'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as FlowMatchEulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "\n",
      "Loading pipeline components...:  67%|██████▋   | 6/9 [00:00<00:00, 57.10it/s]\u001b[ALoaded tokenizer_3 as T5TokenizerFast from `tokenizer_3` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "Loading pipeline components...: 100%|██████████| 9/9 [00:00<00:00, 29.52it/s]\n",
      "07/15/2025 16:41:04 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: a photo of snow covered solar panels.\n",
      "Steps: 100%|██████████| 500/500 [14:40<00:00,  1.12s/it, loss=0.106, lr=0.0001]07/15/2025 16:42:22 - INFO - accelerate.accelerator - Saving current state to /workspace/solar_ml/trained-sd3-snow/checkpoint-500\n",
      "Configuration saved in /workspace/solar_ml/trained-sd3-snow/checkpoint-500/transformer/config.json\n",
      "Model weights saved in /workspace/solar_ml/trained-sd3-snow/checkpoint-500/transformer/diffusion_pytorch_model.safetensors\n",
      "07/15/2025 16:43:06 - INFO - accelerate.checkpointing - Optimizer state saved in /workspace/solar_ml/trained-sd3-snow/checkpoint-500/optimizer.bin\n",
      "07/15/2025 16:43:06 - INFO - accelerate.checkpointing - Scheduler state saved in /workspace/solar_ml/trained-sd3-snow/checkpoint-500/scheduler.bin\n",
      "07/15/2025 16:43:06 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in /workspace/solar_ml/trained-sd3-snow/checkpoint-500/sampler.bin\n",
      "07/15/2025 16:43:06 - INFO - accelerate.checkpointing - Gradient scaler state saved in /workspace/solar_ml/trained-sd3-snow/checkpoint-500/scaler.pt\n",
      "07/15/2025 16:43:06 - INFO - accelerate.checkpointing - Random states saved in /workspace/solar_ml/trained-sd3-snow/checkpoint-500/random_states_0.pkl\n",
      "07/15/2025 16:43:06 - INFO - __main__ - Saved state to /workspace/solar_ml/trained-sd3-snow/checkpoint-500\n",
      "Steps: 100%|██████████| 500/500 [15:24<00:00,  1.12s/it, loss=0.0428, lr=0.0001]{'image_encoder', 'feature_extractor'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/9 [00:00<?, ?it/s]\u001b[A{'mid_block_add_attention'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-3-medium-diffusers/snapshots/ea42f8cef0f178587cf766dc8129abd379c90671/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "\n",
      "Loading pipeline components...:  11%|█         | 1/9 [00:00<00:03,  2.25it/s]\u001b[A{'base_shift', 'max_shift', 'use_beta_sigmas', 'stochastic_sampling', 'base_image_seq_len', 'invert_sigmas', 'use_dynamic_shifting', 'shift_terminal', 'use_exponential_sigmas', 'use_karras_sigmas', 'max_image_seq_len', 'time_shift_type'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as FlowMatchEulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "Loaded text_encoder_2 as CLIPTextModelWithProjection from `text_encoder_2` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "\n",
      "Loading pipeline components...:  44%|████▍     | 4/9 [00:01<00:02,  2.14it/s]\u001b[ALoaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "Loaded text_encoder as CLIPTextModelWithProjection from `text_encoder` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "\n",
      "Loading pipeline components...:  78%|███████▊  | 7/9 [00:02<00:00,  3.54it/s]\u001b[A\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:05<00:05,  5.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.22s/it]\u001b[A\u001b[A\n",
      "Loaded text_encoder_3 as T5EncoderModel from `text_encoder_3` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "\n",
      "Loading pipeline components...:  89%|████████▉ | 8/9 [00:12<00:02,  2.36s/it]\u001b[ALoaded tokenizer_3 as T5TokenizerFast from `tokenizer_3` subfolder of stabilityai/stable-diffusion-3-medium-diffusers.\n",
      "\n",
      "Loading pipeline components...: 100%|██████████| 9/9 [00:12<00:00,  1.43s/it]\u001b[A\n",
      "Configuration saved in /workspace/solar_ml/trained-sd3-snow/vae/config.json\n",
      "Model weights saved in /workspace/solar_ml/trained-sd3-snow/vae/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in /workspace/solar_ml/trained-sd3-snow/transformer/config.json\n",
      "Model weights saved in /workspace/solar_ml/trained-sd3-snow/transformer/diffusion_pytorch_model.safetensors\n",
      "Configuration saved in /workspace/solar_ml/trained-sd3-snow/scheduler/scheduler_config.json\n",
      "Configuration saved in /workspace/solar_ml/trained-sd3-snow/model_index.json\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/9 [00:00<?, ?it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float16.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /workspace/solar_ml/trained-sd3-snow/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of /workspace/solar_ml/trained-sd3-snow.\n",
      "\n",
      "Loading pipeline components...:  11%|█         | 1/9 [00:01<00:08,  1.03s/it]\u001b[ALoaded scheduler as FlowMatchEulerDiscreteScheduler from `scheduler` subfolder of /workspace/solar_ml/trained-sd3-snow.\n",
      "Instantiating SD3Transformer2DModel model under default dtype torch.float16.\n",
      "All model checkpoint weights were used when initializing SD3Transformer2DModel.\n",
      "\n",
      "All the weights of SD3Transformer2DModel were initialized from the model checkpoint at /workspace/solar_ml/trained-sd3-snow/transformer.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use SD3Transformer2DModel for predictions without further training.\n",
      "Loaded transformer as SD3Transformer2DModel from `transformer` subfolder of /workspace/solar_ml/trained-sd3-snow.\n",
      "\n",
      "Loading pipeline components...:  33%|███▎      | 3/9 [00:20<00:44,  7.42s/it]\u001b[ALoaded text_encoder_2 as CLIPTextModelWithProjection from `text_encoder_2` subfolder of /workspace/solar_ml/trained-sd3-snow.\n",
      "\n",
      "Loading pipeline components...:  44%|████▍     | 4/9 [00:26<00:35,  7.02s/it]\u001b[ALoaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of /workspace/solar_ml/trained-sd3-snow.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of /workspace/solar_ml/trained-sd3-snow.\n",
      "\n",
      "Loading pipeline components...:  67%|██████▋   | 6/9 [00:26<00:10,  3.56s/it]\u001b[ALoaded text_encoder as CLIPTextModelWithProjection from `text_encoder` subfolder of /workspace/solar_ml/trained-sd3-snow.\n",
      "\n",
      "Loading pipeline components...:  78%|███████▊  | 7/9 [00:27<00:05,  2.93s/it]\u001b[A\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:28,  9.39s/it]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  50%|█████     | 2/4 [00:16<00:16,  8.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:22<00:07,  7.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:28<00:00,  7.07s/it]\u001b[A\u001b[A\n",
      "Loaded text_encoder_3 as T5EncoderModel from `text_encoder_3` subfolder of /workspace/solar_ml/trained-sd3-snow.\n",
      "\n",
      "Loading pipeline components...:  89%|████████▉ | 8/9 [00:56<00:09,  9.85s/it]\u001b[ALoaded tokenizer_3 as T5TokenizerFast from `tokenizer_3` subfolder of /workspace/solar_ml/trained-sd3-snow.\n",
      "\n",
      "Loading pipeline components...: 100%|██████████| 9/9 [00:56<00:00,  6.27s/it]\u001b[A\n",
      "07/15/2025 16:45:53 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: a photo of snow covered solar panels.\n",
      "Steps: 100%|██████████| 500/500 [18:40<00:00,  2.24s/it, loss=0.0428, lr=0.0001]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -e\n",
    "\n",
    "# 1) Go into your DreamBooth example folder\n",
    "cd /workspace/solar_ml/diffusers/examples/dreambooth\n",
    "\n",
    "# # 2) Install the example deps (this pulls in the old PEFT)\n",
    "# pip install -r requirements_sd3.txt\n",
    "\n",
    "# # 3) Now force‑upgrade PEFT to a compatible version\n",
    "# pip install --upgrade \"peft>=0.15.0\"\n",
    "\n",
    "# 4) Finally relaunch your training\n",
    "accelerate launch train_dreambooth_sd3.py \\\n",
    "  --pretrained_model_name_or_path stabilityai/stable-diffusion-3-medium-diffusers \\\n",
    "  --instance_data_dir /workspace/solar_ml/dataset/snow \\\n",
    "  --output_dir /workspace/solar_ml/trained-sd3-snow \\\n",
    "  --instance_prompt \"a photo of snow covered solar panels\" \\\n",
    "  --resolution 256 \\\n",
    "  --train_batch_size 1 \\\n",
    "  --gradient_accumulation_steps 4 \\\n",
    "  --learning_rate 1e-4 \\\n",
    "  --max_train_steps 500 \\\n",
    "  --use_8bit_adam \\\n",
    "  --validation_prompt \"a photo of snow covered solar panels\" \\\n",
    "  --validation_epochs 25 \\\n",
    "  --mixed_precision fp16 \\\n",
    "  --report_to tensorboard --gradient_checkpointing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a374f63-1ff2-48cf-af8a-9712e0e6bd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall -y diffusers\n",
    "# !git clone https://github.com/huggingface/diffusers.git /workspace/diffusers-main\n",
    "# !pip install -e /workspace/diffusers-main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2504f72c-e4f3-4fc1-87cc-6b14128a6d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32d7b88b-9001-4137-942c-dbc6b289324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/workspace/solar_ml/diffusers/src\")\n",
    "\n",
    "from diffusers import StableDiffusion3Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19533ee0-6fc8-488f-946e-37a7094aa43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/solar_ml/trained-sd3-snow\n",
      "├── checkpoint-500\n",
      "│   ├── optimizer.bin\n",
      "│   ├── random_states_0.pkl\n",
      "│   ├── scaler.pt\n",
      "│   ├── scheduler.bin\n",
      "│   └── transformer\n",
      "│       ├── config.json\n",
      "│       └── diffusion_pytorch_model.safetensors\n",
      "├── logs\n",
      "│   └── dreambooth-sd3\n",
      "│       ├── 1752594885.8671236\n",
      "│       │   └── events.out.tfevents.1752594885.c4eca63a5615.417.1\n",
      "│       ├── 1752594885.8781743\n",
      "│       │   └── hparams.yml\n",
      "│       ├── 1752596861.751855\n",
      "│       │   └── events.out.tfevents.1752596861.c4eca63a5615.1182.1\n",
      "│       ├── 1752596861.806245\n",
      "│       │   └── hparams.yml\n",
      "│       ├── events.out.tfevents.1752594885.c4eca63a5615.417.0\n",
      "│       └── events.out.tfevents.1752596861.c4eca63a5615.1182.0\n",
      "├── model_index.json\n",
      "├── scheduler\n",
      "│   └── scheduler_config.json\n",
      "├── text_encoder\n",
      "│   ├── config.json\n",
      "│   └── model.safetensors\n",
      "├── text_encoder_2\n",
      "│   ├── config.json\n",
      "│   └── model.safetensors\n",
      "├── text_encoder_3\n",
      "│   ├── config.json\n",
      "│   ├── model-00001-of-00004.safetensors\n",
      "│   ├── model-00002-of-00004.safetensors\n",
      "│   ├── model-00003-of-00004.safetensors\n",
      "│   ├── model-00004-of-00004.safetensors\n",
      "│   └── model.safetensors.index.json\n",
      "├── tokenizer\n",
      "│   ├── merges.txt\n",
      "│   ├── special_tokens_map.json\n",
      "│   ├── tokenizer_config.json\n",
      "│   └── vocab.json\n",
      "├── tokenizer_2\n",
      "│   ├── merges.txt\n",
      "│   ├── special_tokens_map.json\n",
      "│   ├── tokenizer_config.json\n",
      "│   └── vocab.json\n",
      "├── tokenizer_3\n",
      "│   ├── special_tokens_map.json\n",
      "│   ├── spiece.model\n",
      "│   ├── tokenizer.json\n",
      "│   └── tokenizer_config.json\n",
      "├── transformer\n",
      "│   ├── config.json\n",
      "│   └── diffusion_pytorch_model.safetensors\n",
      "└── vae\n",
      "    ├── config.json\n",
      "    └── diffusion_pytorch_model.safetensors\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def print_tree(root: str, prefix: str = \"\"):\n",
    "    \"\"\"Recursively prints a tree view of files under `root`.\"\"\"\n",
    "    entries = sorted(os.listdir(root))\n",
    "    pointers = {entry: \"└── \" if i == len(entries) - 1 else \"├── \"\n",
    "                for i, entry in enumerate(entries)}\n",
    "    for entry in entries:\n",
    "        path = os.path.join(root, entry)\n",
    "        print(prefix + pointers[entry] + entry)\n",
    "        if os.path.isdir(path):\n",
    "            extension = \"    \" if pointers[entry] == \"└── \" else \"│   \"\n",
    "            print_tree(path, prefix + extension)\n",
    "\n",
    "# Replace this with the path to your trained-sd3 folder\n",
    "trained_dir = \"/workspace/solar_ml/trained-sd3-snow\"\n",
    "\n",
    "print(trained_dir)\n",
    "print_tree(trained_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64b84b87-aca9-4a0e-bab1-20ed217d0bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b7bf4c434442a783e99d4d7ee60d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7320b87cc94a1e862684b6535e8681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "402fdcc609084283b378f92200e04789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: snow_3.png\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# if your notebook still can’t see diffusers, uncomment:\n",
    "# sys.path.insert(0, \"/workspace/solar_ml/diffusers/src\")\n",
    "\n",
    "from diffusers import StableDiffusion3Pipeline\n",
    "import torch\n",
    "\n",
    "# 1) Point at your trained‑sd3 directory\n",
    "model_dir = \"/workspace/solar_ml/trained-sd3-snow\"\n",
    "\n",
    "# 2) Load the full SD3 pipeline (it already contains your fine‑tuned weights)\n",
    "pipe = StableDiffusion3Pipeline.from_pretrained(\n",
    "    model_dir,\n",
    "    torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "# 3) Generate!\n",
    "prompt = \"snow covered solar panels\"\n",
    "out = pipe(\n",
    "    prompt,\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=7.5,\n",
    ")\n",
    "image = out.images[0]\n",
    "\n",
    "# 4) Save to disk\n",
    "image.save(\"snow_3.png\")\n",
    "print(\"✅ Saved: snow_3.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5effc092-443c-4843-9114-aacf08fdd8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/solar_ml/diffusers/examples/dreambooth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26a8e8b1-94ea-4d8f-9159-f18cee38727a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6f1428b34d4f338c8eceaf3fcbddad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Pipeline <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> expected ['feature_extractor', 'image_encoder', 'safety_checker', 'scheduler', 'text_encoder', 'tokenizer', 'unet', 'vae'], but only {'text_encoder', 'tokenizer', 'scheduler', 'vae'} were passed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 1. Load the base model (no LoRA adapters applied)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m pipe2 \u001b[38;5;241m=\u001b[39m \u001b[43mStableDiffusionPipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/workspace/solar_ml/sd3_local\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfp16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 2. If you’d previously loaded LoRA weights, make sure they’re fully removed:\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#    Uncomment one of these two lines depending on your version of diffusers:\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# 3. Generate your image (pure base model)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma photo of snow covered solar panels\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/solar_ml/diffusers/src/diffusers/pipelines/pipeline_utils.py:1072\u001b[0m, in \u001b[0;36mDiffusionPipeline.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_modules) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1071\u001b[0m     passed_modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mlist\u001b[39m(init_kwargs\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(passed_class_obj\u001b[38;5;241m.\u001b[39mkeys())) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(optional_kwargs)\n\u001b[0;32m-> 1072\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1073\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpipeline_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m were passed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1074\u001b[0m     )\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;66;03m# 10. Type checking init arguments\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m kw, arg \u001b[38;5;129;01min\u001b[39;00m init_kwargs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1078\u001b[0m     \u001b[38;5;66;03m# Too complex to validate with type annotation alone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Pipeline <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> expected ['feature_extractor', 'image_encoder', 'safety_checker', 'scheduler', 'text_encoder', 'tokenizer', 'unet', 'vae'], but only {'text_encoder', 'tokenizer', 'scheduler', 'vae'} were passed."
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "# 1. Load the base model (no LoRA adapters applied)\n",
    "pipe2 = StableDiffusionPipeline.from_pretrained(\n",
    "    \"/workspace/solar_ml/sd3_local\",\n",
    "    torch_dtype=torch.float16,\n",
    "    revision=\"fp16\",\n",
    "    use_safetensors=True,\n",
    "    local_files_only=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "# 2. If you’d previously loaded LoRA weights, make sure they’re fully removed:\n",
    "#    Uncomment one of these two lines depending on your version of diffusers:\n",
    "\n",
    "# pipe.unload_lora_weights()  # for diffusers ≥0.19.0\n",
    "# pipe.disable_lora()         # alternative alias in some releases\n",
    "\n",
    "# 3. Generate your image (pure base model)\n",
    "prompt = \"a photo of snow covered solar panels\"\n",
    "result = pipe2(prompt, guidance_scale=7.5, num_inference_steps=50)\n",
    "\n",
    "# 4. Save or display\n",
    "image = result.images[0]\n",
    "image.save(\"snow_panels_base.png\")\n",
    "print(\"Saved → snow_panels_base.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24015d3-2d4e-41f8-a425-09bd5f2bf24e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
